{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id=\"TreasureGuardian-v0\",\n",
    "    entry_point=r\"C:\\Users\\malli\\Desktop\\Sem_6\\RL\\The Treasure Gaurdian\\env.py:BaseTreasureGuardianEnv\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from copy import deepcopy\n",
    "from env import LightTreasureGuardianEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🕹️ 3. Define Your Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim)  # Output logits for discrete actions\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.fc(obs)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, total_obs_dim, total_act_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(total_obs_dim + total_act_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_cat, act_cat):\n",
    "        x = torch.cat([obs_cat, act_cat], dim=-1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤖 4. MADDPG Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPGAgent:\n",
    "    def __init__(self, obs_dim, act_dim, total_obs_dim, total_act_dim, lr=1e-3):\n",
    "        self.actor = Actor(obs_dim, act_dim)\n",
    "        self.target_actor = deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
    "\n",
    "        self.critic = Critic(total_obs_dim, total_act_dim)\n",
    "        self.target_critic = deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "        logits = self.actor(obs_tensor)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "\n",
    "    def update(self, batch, agents, agent_idx, gamma=0.95, tau=0.01):\n",
    "        obs_n, act_n, rew_n, next_obs_n, done_n = batch\n",
    "\n",
    "        obs = torch.FloatTensor([o[agent_idx] for o in obs_n])\n",
    "        actions = torch.LongTensor([a[agent_idx] for a in act_n]).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor([r[agent_idx] for r in rew_n])\n",
    "        dones = torch.FloatTensor(done_n)\n",
    "\n",
    "        obs_cat = torch.FloatTensor([np.concatenate(o) for o in obs_n])\n",
    "        act_cat = torch.FloatTensor([a for a in act_n])\n",
    "\n",
    "        # Critic Update\n",
    "        q = self.critic(obs_cat, act_cat).squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_obs_cat = torch.FloatTensor([np.concatenate(o) for o in next_obs_n])\n",
    "            next_actions = torch.stack([\n",
    "                torch.softmax(agents[i].target_actor(torch.FloatTensor(o[i]).unsqueeze(0)), dim=-1).argmax(dim=-1).float()\n",
    "                for i in range(len(agents))\n",
    "                for o in [next_obs_n]\n",
    "            ], dim=-1)\n",
    "            target_q = self.target_critic(next_obs_cat, next_actions)\n",
    "            target_q = rewards + gamma * target_q.squeeze() * (1 - dones)\n",
    "\n",
    "        critic_loss = F.mse_loss(q, target_q)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor Update\n",
    "        pred_actions = act_cat.clone()\n",
    "        pred_actions[:, agent_idx] = torch.softmax(self.actor(obs), dim=-1).argmax(dim=-1).float()\n",
    "        actor_loss = -self.critic(obs_cat, pred_actions).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Soft Update\n",
    "        self._soft_update(self.actor, self.target_actor, tau)\n",
    "        self._soft_update(self.critic, self.target_critic, tau)\n",
    "\n",
    "    def _soft_update(self, main, target, tau):\n",
    "        for tp, mp in zip(target.parameters(), main.parameters()):\n",
    "            tp.data.copy_(tp.data * (1 - tau) + mp.data * tau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📥 5. Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎮 6. Initialize Agents, Environment, and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Replace this with your environment import or registration if needed\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTreasureGuardian-v0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m agents \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     MADDPGAgent(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m),  \u001b[38;5;66;03m# Guardian\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     MADDPGAgent(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m)   \u001b[38;5;66;03m# Villains\u001b[39;00m\n\u001b[0;32m      7\u001b[0m ]\n\u001b[0;32m      9\u001b[0m buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\envs\\registration.py:702\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    699\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 702\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[0;32m    705\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gymnasium\\envs\\registration.py:548\u001b[0m, in \u001b[0;36mload_env_creator\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_env_creator\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EnvCreator \u001b[38;5;241m|\u001b[39m VectorEnvCreator:\n\u001b[0;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m        The environment constructor for the given environment name.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    549\u001b[0m     mod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(mod_name)\n\u001b[0;32m    550\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Replace this with your environment import or registration if needed\n",
    "env = gym.make(\"TreasureGuardian-v0\")\n",
    "\n",
    "agents = [\n",
    "    MADDPGAgent(2, 4, 4, 2),  # Guardian\n",
    "    MADDPGAgent(2, 4, 4, 2)   # Villains\n",
    "]\n",
    "\n",
    "buffer = ReplayBuffer()\n",
    "episodes = 1000\n",
    "batch_size = 64\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    obs_n = [obs[\"guardian\"], obs[\"villains\"]]\n",
    "    ep_reward = [0, 0]\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        actions = [agent.get_action(o) for agent, o in zip(agents, obs_n)]\n",
    "        step_action = {\n",
    "            \"guardian\": actions[0],\n",
    "            \"villains\": np.array(actions[1])\n",
    "        }\n",
    "\n",
    "        next_obs, reward, done, _, _ = env.step(step_action)\n",
    "        next_obs_n = [next_obs[\"guardian\"], next_obs[\"villains\"]]\n",
    "        reward_n = [reward[0], reward[1]]\n",
    "\n",
    "        buffer.add((obs_n, actions, reward_n, next_obs_n, [int(done)]))\n",
    "        obs_n = next_obs_n\n",
    "        terminated = done\n",
    "\n",
    "        for i in range(2):\n",
    "            ep_reward[i] += reward_n[i]\n",
    "\n",
    "        if len(buffer) >= batch_size:\n",
    "            batch = buffer.sample(batch_size)\n",
    "            for i in range(2):\n",
    "                agents[i].update(batch, agents, i)\n",
    "\n",
    "    print(f\"Episode {ep}: Guardian: {ep_reward[0]} | Villains: {ep_reward[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💾 7. Save Models (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agents[0].actor.state_dict(), \"guardian_actor.pth\")\n",
    "torch.save(agents[1].actor.state_dict(), \"villains_actor.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
