{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from env import TreasureGuardianEnv  # Import your game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replay Buffer\n",
    "\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"actions\", \"rewards\", \"next_state\", \"done\"])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, actions, rewards, next_state, done):\n",
    "        self.buffer.append(Transition(state, actions, rewards, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multi-Agent Training\n",
    "class MultiAgentReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MADDPG\n",
    "\n",
    "def train_maddpg(env, n_episodes=500, batch_size=128):\n",
    "    n_agents = env.n_agents\n",
    "    agents = [MADDPGAgent(env.state_dim, env.action_dim) for _ in range(n_agents)]\n",
    "    replay_buffer = MultiAgentReplayBuffer()\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = [agent.actor(obs[i]) for i, agent in enumerate(agents)]\n",
    "            next_obs, rewards, done, _ = env.step(actions)\n",
    "            replay_buffer.add((obs, actions, rewards, next_obs, done))\n",
    "            \n",
    "            if replay_buffer.size() > batch_size:\n",
    "                samples = replay_buffer.sample(batch_size)\n",
    "                for idx, agent in enumerate(agents):\n",
    "                    agent.update(samples, agents, idx)\n",
    "            \n",
    "            obs = next_obs\n",
    "        print(f\"Episode {episode+1} completed\")\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Neural Network Models\n",
    "\n",
    "def build_actor(obs_dim, act_dim, hidden_dim=64):\n",
    "    inputs = Input(shape=(obs_dim,))\n",
    "    x = Dense(hidden_dim, activation='relu')(inputs)\n",
    "    x = Dense(hidden_dim, activation='relu')(x)\n",
    "    outputs = Dense(act_dim, activation='softmax')(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "def build_critic(state_dim, total_act_dim, hidden_dim=128):\n",
    "    inputs = Input(shape=(state_dim + total_act_dim,))\n",
    "    x = Dense(hidden_dim, activation='relu')(inputs)\n",
    "    x = Dense(hidden_dim, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MADDPG Agent\n",
    "\n",
    "class MADDPGAgent:\n",
    "    def __init__(self, agent_id, obs_dim, act_dim, state_dim, total_act_dim,\n",
    "                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.95, tau=0.01):\n",
    "        self.agent_id = agent_id\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.actor = build_actor(obs_dim, act_dim)\n",
    "        self.critic = build_critic(state_dim, total_act_dim)\n",
    "        self.target_actor = build_actor(obs_dim, act_dim)\n",
    "        self.target_critic = build_critic(state_dim, total_act_dim)\n",
    "        \n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        self.actor_optimizer = Adam(learning_rate=actor_lr)\n",
    "        self.critic_optimizer = Adam(learning_rate=critic_lr)\n",
    "    \n",
    "    def select_action(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        action_probs = self.actor.predict(obs, verbose=0)[0]\n",
    "        return np.argmax(action_probs), action_probs\n",
    "    \n",
    "    def update(self, samples, all_agents, agent_index):\n",
    "        states, actions, rewards, next_states, dones = samples\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        actions_tensor = np.concatenate(actions, axis=1)\n",
    "        \n",
    "        next_actions = [agent.target_actor.predict(next_states, verbose=0) for agent in all_agents]\n",
    "        next_actions_cat = np.concatenate(next_actions, axis=1)\n",
    "        target_q = self.target_critic.predict(np.hstack([next_states, next_actions_cat]), verbose=0)\n",
    "        y = rewards[:, agent_index].reshape(-1, 1) + self.gamma * target_q * (1 - dones.reshape(-1, 1))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.critic(np.hstack([states, actions_tensor]), training=True)\n",
    "            critic_loss = tf.reduce_mean(tf.square(y - q_values))\n",
    "        grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            current_action = self.actor(states, training=True)\n",
    "            actions_modified = np.copy(actions)\n",
    "            actions_modified[agent_index] = current_action\n",
    "            actions_concat = np.concatenate(actions_modified, axis=1)\n",
    "            actor_loss = -tf.reduce_mean(self.critic(np.hstack([states, actions_concat])))\n",
    "        grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
    "        \n",
    "        for target_param, param in zip(self.target_actor.trainable_variables, self.actor.trainable_variables):\n",
    "            target_param.assign(self.tau * param + (1 - self.tau) * target_param)\n",
    "        for target_param, param in zip(self.target_critic.trainable_variables, self.critic.trainable_variables):\n",
    "            target_param.assign(self.tau * param + (1 - self.tau) * target_param)\n",
    "        \n",
    "        return critic_loss.numpy(), actor_loss.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop\n",
    "\n",
    "def train_maddpg(env, n_episodes=500, batch_size=128):\n",
    "    replay_buffer = ReplayBuffer()\n",
    "    init_obs = env.reset()\n",
    "    state_dim = np.prod(init_obs.shape)  # Ensure correct shape\n",
    "    act_dim = 4  # Discrete actions\n",
    "    total_agents = 1 + env.num_villains\n",
    "    total_act_dim = total_agents * act_dim\n",
    "    \n",
    "    agents = [MADDPGAgent(i, state_dim, act_dim, state_dim, total_act_dim) for i in range(total_agents)]\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        global_state = obs.flatten()\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            actions_list = []\n",
    "            actions_onehots = []\n",
    "            for agent in agents:\n",
    "                a_int, a_onehot = agent.select_action(global_state)\n",
    "                actions_list.append(a_int)\n",
    "                actions_onehots.append(a_onehot)\n",
    "            \n",
    "            env_actions = {\"guardian\": actions_list[0], \"villains\": np.array(actions_list[1:])}\n",
    "            next_obs, rewards, done, _ = env.step(env_actions)  # Handle different env output\n",
    "            \n",
    "            replay_buffer.add(global_state, np.array(actions_onehots), np.array(rewards), next_obs.flatten(), done)\n",
    "            global_state = next_obs.flatten()\n",
    "            step += 1\n",
    "            \n",
    "            if len(replay_buffer) > batch_size and step % 100 == 0:\n",
    "                samples = replay_buffer.sample(batch_size)\n",
    "                for idx, agent in enumerate(agents):\n",
    "                    agent.update(samples, agents, idx)\n",
    "        \n",
    "        print(f\"Episode {episode+1} completed\")\n",
    "    \n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'observation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m init_obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# If reset() returns a dict, extract relevant state\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(init_obs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     init_obs \u001b[38;5;241m=\u001b[39m \u001b[43minit_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Change this key based on your env structure\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m     11\u001b[0m     trained_agents \u001b[38;5;241m=\u001b[39m train_maddpg(env, n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'observation'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = TreasureGuardianEnv(grid_size=10, num_villains=3, num_keys=5, num_pits=3, render_mode=None, max_steps=200)\n",
    "    device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "\n",
    "    # Ensure proper observation retrieval\n",
    "    init_obs = env.reset()  # If reset() returns a dict, extract relevant state\n",
    "    if isinstance(init_obs, dict):\n",
    "        init_obs = init_obs[\"observation\"]  # Change this key based on your env structure\n",
    "\n",
    "    with tf.device(device):\n",
    "        trained_agents = train_maddpg(env, n_episodes=500, batch_size=128)\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
