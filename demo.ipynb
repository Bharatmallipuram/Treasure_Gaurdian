{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from env import TreasureGuardianEnv\n",
    "from maddpg import MADDPG\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Dict('guardian': Box(0, 9, (2,), int32), 'keys': Box(0, 9, (3, 2), int32), 'pits': Box(0, 9, (2, 2), int32), 'treasure': Box(0, 9, (2,), int32), 'villains': Box(0, 9, (1, 2), int32), 'walls': Box(0, 9, (15, 2), int32))\n",
      "Observation Space Type: <class 'gymnasium.spaces.dict.Dict'>\n"
     ]
    }
   ],
   "source": [
    "env = TreasureGuardianEnv()\n",
    "MAX_VILLAINS = 2  # or whatever maximum number of villains you want to support\n",
    "MAX_KEYS = 3          # max number of keys that can appear in the game\n",
    "MAX_WALLS = 20        # estimate based on your map\n",
    "MAX_PITS = 5          # estimated upper bound on pit count\n",
    "\n",
    "\n",
    "print(\"Observation Space:\", env.observation_space)\n",
    "print(\"Observation Space Type:\", type(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m TreasureGuardianEnv()\n\u001b[0;32m      2\u001b[0m num_agents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_villains  \u001b[38;5;66;03m# Guardian + Villains\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m maddpg \u001b[38;5;241m=\u001b[39m \u001b[43mMADDPG\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_agents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMADDPG initialized successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\malli\\Desktop\\Sem_6\\RL\\The Treasure Gaurdian\\maddpg.py:122\u001b[0m, in \u001b[0;36mMADDPG.__init__\u001b[1;34m(self, env, num_agents, buffer_size, batch_size, update_every)\u001b[0m\n\u001b[0;32m    119\u001b[0m total_obs_dim \u001b[38;5;241m=\u001b[39m obs_dim \u001b[38;5;241m*\u001b[39m num_agents\n\u001b[0;32m    120\u001b[0m total_action_dim \u001b[38;5;241m=\u001b[39m action_dim \u001b[38;5;241m*\u001b[39m num_agents\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_obs_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_action_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(buffer_size)\n",
      "File \u001b[1;32mc:\\Users\\malli\\Desktop\\Sem_6\\RL\\The Treasure Gaurdian\\maddpg.py:66\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[1;34m(self, obs_dim, action_dim, total_obs_dim, total_action_dim, lr, tau, gamma)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs_dim, action_dim, total_obs_dim, total_action_dim, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m \u001b[43mActor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_actor \u001b[38;5;241m=\u001b[39m Actor(obs_dim, action_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic \u001b[38;5;241m=\u001b[39m Critic(total_obs_dim, total_action_dim)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\malli\\Desktop\\Sem_6\\RL\\The Treasure Gaurdian\\maddpg.py:19\u001b[0m, in \u001b[0;36mActor.__init__\u001b[1;34m(self, obs_dim, action_dim)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs_dim, action_dim):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Actor, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m     15\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(obs_dim, \u001b[38;5;241m128\u001b[39m),   \u001b[38;5;66;03m# âœ… Use obs_dim here\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m     17\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m),\n\u001b[0;32m     18\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m---> 19\u001b[0m         \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:106\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "env = TreasureGuardianEnv()\n",
    "num_agents = 1 + env.num_villains  # Guardian + Villains\n",
    "maddpg = MADDPG(env=env, num_agents=num_agents)\n",
    "print(\"MADDPG initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Keys: dict_keys(['guardian', 'keys', 'pits', 'treasure', 'villains', 'walls'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation Space Keys:\", env.observation_space.spaces.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 3000\n",
    "max_steps = 100\n",
    "reward_log = []\n",
    "\n",
    "def pad_villain_obs(villain_obs, max_villains=MAX_VILLAINS):\n",
    "    villain_obs = np.array(villain_obs)\n",
    "\n",
    "    if villain_obs.ndim == 1:\n",
    "        villain_obs = np.expand_dims(villain_obs, axis=0)\n",
    "\n",
    "    k = villain_obs.shape[0]\n",
    "    obs_dim = villain_obs.shape[1] if villain_obs.ndim == 2 else 0\n",
    "\n",
    "    if k < max_villains:\n",
    "        pad = np.full((max_villains - k, obs_dim), -1, dtype=villain_obs.dtype)\n",
    "        return np.concatenate([villain_obs, pad], axis=0)\n",
    "    elif k > max_villains:\n",
    "        return villain_obs[:max_villains]  # Optional truncation\n",
    "    else:\n",
    "        return villain_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_obs(obs_dict, max_villains, max_keys, max_walls, max_pits):\n",
    "    def pad(arr, max_len):\n",
    "        if len(arr) == 0:\n",
    "            return np.zeros((max_len, 2), dtype=np.float32)\n",
    "        return np.vstack([arr, np.zeros((max_len - len(arr), 2), dtype=np.float32)])[:max_len]\n",
    "\n",
    "    guardian = np.array(obs_dict['guardian'], dtype=np.float32).flatten()\n",
    "    villains = pad(obs_dict['villains'], max_villains).flatten()\n",
    "    keys = pad(obs_dict['keys'], max_keys).flatten()\n",
    "    walls = pad(obs_dict['walls'], max_walls).flatten()\n",
    "    pits = pad(obs_dict['pits'], max_pits).flatten()\n",
    "    treasure = np.array(obs_dict['treasure'], dtype=np.float32).flatten()\n",
    "    \n",
    "    return np.concatenate([guardian, villains, keys, walls, pits, treasure])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guardian_obs:  {'guardian': array([0, 0]), 'villains': array([[2, 4]]), 'keys': array([[0, 2],\n",
      "       [3, 4],\n",
      "       [1, 1]]), 'walls': array([[0, 1],\n",
      "       [4, 0],\n",
      "       [6, 0],\n",
      "       [4, 9],\n",
      "       [9, 6],\n",
      "       [0, 3],\n",
      "       [9, 2],\n",
      "       [7, 3],\n",
      "       [7, 6],\n",
      "       [5, 0],\n",
      "       [3, 6],\n",
      "       [6, 6],\n",
      "       [5, 9],\n",
      "       [3, 2],\n",
      "       [1, 9]]), 'treasure': array([2, 1]), 'pits': array([[9, 7],\n",
      "       [3, 3]])}\n",
      "villains_obs_list []\n",
      "guardian_obs is a dict. Keys: dict_keys(['guardian', 'villains', 'keys', 'walls', 'treasure', 'pits'])\n",
      "Guardian obs shape: (64,)\n",
      "Villain obs shape: (64,)\n",
      "Full obs shape: (3, 64)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'maddpg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull obs shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39marray(obs)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m---> 38\u001b[0m     action_list \u001b[38;5;241m=\u001b[39m \u001b[43mmaddpg\u001b[49m\u001b[38;5;241m.\u001b[39mact(obs)\n\u001b[0;32m     40\u001b[0m     actions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mguardian\u001b[39m\u001b[38;5;124m\"\u001b[39m: action_list[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvillains\u001b[39m\u001b[38;5;124m\"\u001b[39m: action_list[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_villains]\n\u001b[0;32m     43\u001b[0m     }\n\u001b[0;32m     45\u001b[0m     next_obs_raw, rewards_raw, done_raw, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'maddpg' is not defined"
     ]
    }
   ],
   "source": [
    "for ep in range(1, n_episodes + 1):\n",
    "    obs_raw = env.reset()\n",
    "    guardian_obs, villains_obs_dict = obs_raw\n",
    "\n",
    "    # Ensure villain observations are converted to list format\n",
    "    villains_obs_list = [\n",
    "        np.array(villains_obs_dict[i], dtype=np.float32)  # ensure float array\n",
    "        for i in sorted(villains_obs_dict.keys())\n",
    "    ]\n",
    "\n",
    "    print(\"guardian_obs: \", guardian_obs)\n",
    "    print(\"villains_obs_list\", villains_obs_list)\n",
    "    \n",
    "    # Padding (if needed) â€” already a list of arrays now\n",
    "    villains_obs = pad_villain_obs(villains_obs_list, MAX_VILLAINS)\n",
    "\n",
    "    # Check if guardian_obs is a dict\n",
    "    if isinstance(guardian_obs, dict):\n",
    "        print(\"guardian_obs is a dict. Keys:\", guardian_obs.keys())\n",
    "    \n",
    "    guardian_flat = flatten_obs(guardian_obs, MAX_VILLAINS, MAX_KEYS, MAX_WALLS, MAX_PITS)\n",
    "    villains_flat = [flatten_obs(v_obs, MAX_VILLAINS, MAX_KEYS, MAX_WALLS, MAX_PITS) for v_obs in villains_obs_list]\n",
    "    \n",
    "    # Pad if fewer villains\n",
    "    while len(villains_flat) < MAX_VILLAINS:\n",
    "        villains_flat.append(np.zeros_like(guardian_flat))\n",
    "    \n",
    "    obs = [guardian_flat] + villains_flat\n",
    "    \n",
    "    total_reward = np.zeros(1 + MAX_VILLAINS)\n",
    "\n",
    "    print(\"Guardian obs shape:\", guardian_flat.shape)\n",
    "    print(\"Villain obs shape:\", villains_flat[0].shape)\n",
    "    print(\"Full obs shape:\", np.array(obs).shape)\n",
    "    \n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action_list = maddpg.act(obs)\n",
    "\n",
    "        actions = {\n",
    "            \"guardian\": action_list[0],\n",
    "            \"villains\": action_list[1:1 + env.num_villains]\n",
    "        }\n",
    "\n",
    "        next_obs_raw, rewards_raw, done_raw, _ = env.step(actions)\n",
    "        guardian_next, villains_next = next_obs_raw\n",
    "\n",
    "        villains_next = pad_villain_obs(villains_next, MAX_VILLAINS)\n",
    "        next_obs = [guardian_next] + [villains_next[i] for i in range(MAX_VILLAINS)]\n",
    "\n",
    "        # Unpack rewards\n",
    "        guardian_reward, villain_reward = rewards_raw\n",
    "        rewards = [guardian_reward] + [villain_reward for _ in range(MAX_VILLAINS)]\n",
    "\n",
    "        done = [done_raw] * (1 + MAX_VILLAINS)\n",
    "\n",
    "        maddpg.step(obs, action_list, rewards, next_obs, done)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += np.array(rewards)\n",
    "\n",
    "        if any(done):\n",
    "            break\n",
    "\n",
    "    reward_log.append(total_reward)\n",
    "\n",
    "    if ep % 100 == 0:\n",
    "        avg_rewards = np.mean(reward_log[-100:], axis=0)\n",
    "        print(f\"Episode {ep} - Avg Reward: {avg_rewards}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Plot Rewards\n",
    "\n",
    "reward_log = np.array(reward_log)\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_agents):\n",
    "    plt.plot(reward_log[:, i], label=f\"Agent {i}\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Training Rewards per Agent\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Save Trained Models\n",
    "maddpg.save(\"maddpg_models/\")\n",
    "print(\"Models saved to 'maddpg_models/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
